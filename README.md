# Redpanda-streaming-tickets
Ce projet met en place une pipeline de traitement de tickets clients en temps rÃ©el, basÃ©e sur un Ã©cosystÃ¨me **Kafka (Redpanda)**, **Apache Spark Structured Streaming** et **AWS S3**. Lâ€™architecture est conteneurisÃ©e via **Docker Compose**.

## ğŸ“ PrÃ©requis

Avant de dÃ©marrer, assure-toi dâ€™avoir :

- Docker & Docker Compose installÃ©s sur ta machine
- Un compte AWS valide
- Un bucket S3 existant
- Une paire de clÃ©s d'accÃ¨s AWS (Access Key ID + Secret Access Key)
- [Facultatif] Un environnement Python local si tu veux tester certains scripts sans Docker

---

## ğŸ“¦ Architecture


## ğŸ§± Composants
- **Redpanda (Kafka-compatible)** : cluster 3 nÅ“uds pour le streaming

- **Apache Spark** : cluster 1 master / 1 worker pour le traitement des flux

- **Producteur Python** : gÃ©nÃ¨re des tickets alÃ©atoires et les publie dans Kafka

- **Consumer PySpark** : agrÃ¨ge les donnÃ©es et les Ã©crit dans AWS S3 (format Parquet)

- **Jupyter Notebook** : lit les donnÃ©es depuis S3 pour les analyser

- **Docker Compose** : orchestration complÃ¨te de l'infrastructure

## ğŸš€ Lancement du projet
1. Cloner le repo
   ```bash
   git clone https://github.com/zaracky/Redpanda-streaming-tickets.git
   
 2. Configurer les variables d'environnement

Modifier le fichier .env Ã  la racine du projet avec vos informations

3. Lancer tous les services
   ```bash
   docker-compose up --build

Cela va :

Lancer Redpanda, Spark Master & Worker

DÃ©marrer le producteur Kafka (gÃ©nÃ©ration de tickets)

Lancer le consumer PySpark (streaming + stockage)

DÃ©marrer Jupyter Notebook pour visualiser les rÃ©sultats

## ğŸ” AccÃ¨s et Surveillance
Voici les interfaces disponibles une fois les services lancÃ©s :

- Console Redpanda : http://localhost:8080

- Interface Spark Master (UI) : http://localhost:8081

- Jupyter Notebook : http://localhost:8888


## ğŸ—‚ï¸ Structure du Projet

      â”œâ”€â”€ Docker/
         â”œâ”€â”€ Dockerfile.generator        
         â”œâ”€â”€ Dockerfile.notebook        
         â”œâ”€â”€ Dockerfile.pyspark          
         â”œâ”€â”€ docker-compose.yml           
         â””â”€â”€ requirements_pyspark.txt
         â””â”€â”€ requirements_pyspark.txt
         â””â”€â”€ requirements.txt
      â”œâ”€â”€ .env
      â”œâ”€â”€ Spark_traitement.py
      â”œâ”€â”€ create_ticket.py
      â”œâ”€â”€ notebook_analysis.ipynb
      â”œâ”€â”€ README.md

## Description des Composants
- Docker/ : Contient tous les Dockerfiles nÃ©cessaires Ã  chaque composant de la pipeline ainsi que le fichier docker-compose.yml pour tout orchestrer.

- .env : Fichier contenant les variables sensibles et de configuration (non versionnÃ© !).

- Spark_traitement.py : Script Spark Streaming lisant les flux Kafka, enrichissant et sauvegardant les donnÃ©es dans AWS S3.

- create_ticket.py : GÃ©nÃ©rateur de donnÃ©es simulant des tickets clients.

- notebook_analysis.ipynb : Analyse visuelle et statistique des donnÃ©es de tickets stockÃ©es en Parquet dans S3.

- README.md : Documentation complÃ¨te du projet.

## DÃ©monstration
